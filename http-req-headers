#!/bin/bash


# Copyright (C) 2020 drpawpaw. All Rights Reserved.
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.


# Provides a local web browser's standard request headers, for use with tools
# like crul, wget and lwp-request (GET). It avoids the need to hard code HTTP
# request headers or remember to periodically revisit their values.
#
# Currently very chromium centric.
#
# Currently depends on having a display.
#
# Designed to be sourced or executed. When executed, capture its stdout for
# the browser's request headers, one per line. Sourcing repeatedly from the
# same runtime context is safe, and will only print the browser headers to
# stdout the first time.
#
# When sourced the browser's headers are available in var HTTP_REQ_HEADS
# as new line delimited strings. Access them with new lines active with
# 'echo -e $HTTP_REQ_HEADS'.
#
# When sourced, use methods like setCurlHeadersInPseudoRandomOrder, capturing
# stdout, for a line of curl cmd-line-formatted HTTP headers in a "random"
# order. See method's comments for "randomness" reason. curl often needs (or
# at least is most easily) run from eval when using these formatted headers.
#
# When calling methods like setCurlHeadersInPseudoRandomOrder, pass in the
# case-sensitive exact names of HTTP request headers (excluding ':' suffix)
# to be excluded from its return value, if any. Do this each time the
# method's called.
#
# setCurlHeadersInPseudoRandomOrder default behavior is to return a different
# (random) User-Agent value each time it's called. To force it instead to
# return the same value for User-Agent with each call, use it something like
# this:
#
#     . http-req-headers &>/dev/null
#     ua=$( randomUserAgentStringValue )
#     [loop]
#         reqHeads=$( setCurlHeadersInPseudoRandomOrder -u "$ua" )
#         eval curl ... $reqHeads ...
#         ...
#     [end loop]
#
# The 'GET' and 'Host' HTTP request headers are never included in those
# produced by this script.
#
# It only pulls headers from the browser once per day. All other same-day
# calls of this script pull headers from file
#
#     $HOME/.[first token of browser var]-http-req-heads
#
# This file's also refreshed when it DNE or is empty.


if [ function != "$( type -t randomUserAgentStringValue )" ]
then
    function randomUserAgentStringValue
    {
        # https://developers.whatismybrowser.com/useragents/explore/
        declare -r \
               userAgentStrings="$HOME/bin/config/http-user-agent-strings.txt"

        if [ ! -f "$userAgentStrings" ] || [ ! -r "$userAgentStrings" ]
        then
            echo -e "\nCan't read file '$userAgentStrings'.\n" >&2
            return 1
        fi

        local userAgent=$( shuf -n 1 "$userAgentStrings" )
        if [ -z "$userAgent" ]
        then
            echo -e "\nFailed to determine a User-Agent value. Exiting.\n" >&2
            exit 1
        fi
        echo "$userAgent"
    }
    export -f randomUserAgentStringValue
fi


# Call setCurlHeadersInPseudoRandomOrder as part of setting up each HTTP
# request that uses the HTTP_REQ_HEADS string produced by this script. Chrome
# sends its basic/standard HTTP request headers in (what what appears to be) a
# random order for each request. This function mimics that behavior.
#
# Note: One context in which Chrome always returns its default HTTP req headers
# in a specific and predictable order is the first GET request it makes after
# starting. It's each subsequent request it makes that orders them "randomly".
#
# User-Agent for the web proxy is also set by this method.
#
# TODO: Study the ordering of Chrome HTTP request headers over a series of
#       requests to determine if there's a pattern. If so, update this method
#       to mimic it.
if [ function != "$( type -t setCurlHeadersInPseudoRandomOrder )" ]
then
    function setCurlHeadersInPseudoRandomOrder
    {
        local excluded=""
        local userAgent=""

        while getopts ":u:e:" arg
        do
            case $arg in

                u  ) userAgent="$OPTARG" ;;

                e  ) [ -n "$excluded" ] && excluded="${excluded}|"
                     excluded="$excluded$OPTARG" ;;

                \? ) echo "Usage: $ ${0##*/}" \
                          "[-u userAgent] [-e HTTP req header to excluded]..."
                     exit 1 ;;
            esac
        done

        [ -z "$userAgent" ] && userAgent=$( randomUserAgentStringValue )
        if [ -z "$userAgent" ]
        then
            echo -e "\nFailed to set HTTP User-Agent value. Exiting." >&2
            exit 1
        fi

        # Either curl or the web proxy end up sending the User-Agent string
        # twice if curl's proxy-header cmd line arg is used. Don't use it.
        local hdr=""
        local headers=""
        local ua=""
        declare IFS=$'\n'
        for hdr in $( echo -e $HTTP_REQ_HEADS | shuf )
        do
            if echo "$hdr" | grep -vE "^($excluded): " &>/dev/null
            then
                if echo "$hdr" | grep -E '^[Uu]ser-[Aa]gent: ' &>/dev/null
                then
                    ua="$( echo "$hdr" |
                                 grep -Eo '^[Uu]ser-[Aa]gent: ' )$userAgent"
                    headers="$headers -H '$ua'"
                else
                    headers="$headers -H '$hdr'"
                fi
            fi
        done
        echo "$headers"
    }
    export -f setCurlHeadersInPseudoRandomOrder
fi


if [ function != "$( type -t setHttpRequestHeaders )" ]
then
    function setHttpRequestHeaders
    {
        local IFS=$'\n'
        local L
        declare -i I=0

        for L in $( < "$HTTP_REQ_HEADS_FILE" )
        do
            if [ -n "$L" ] && echo "$L" | grep -vE "^(GET|Host):? " &>/dev/null
            then
                L=$( echo "$L" | tr -d '\r' )
                [ -z "$L" ] && continue

                # SDCH compression was removed from Chrome in version 59, so
                # even if it's available in the browser/version we're
                # imitating, we don't use it:
                L=$( echo "$L" | sed -r 's/ +sdch,?//g' )
                HTTP_REQ_HEADS="$HTTP_REQ_HEADS$L\n"
                echo "$L"
                I=1+I
            fi
        done
        export HTTP_REQ_HEADS
    }
    export -f setHttpRequestHeaders
fi


[ -z "$BROWSER" ] && declare -r BROWSER=chromium

[ -z "$HTTP_REQ_HEADS_FILE" ] && \
    declare -r HTTP_REQ_HEADS_FILE="$HOME/.$( echo "$BROWSER"  |
                                              sed -r 's|^  *||' |
                                              cut -d ' ' -f1 )-http-req-heads"

[ ! -f "$HTTP_REQ_HEADS_FILE" ] && touch "$HTTP_REQ_HEADS_FILE"

if [ ! -r "$HTTP_REQ_HEADS_FILE" ] || [ ! -w "$HTTP_REQ_HEADS_FILE" ]
then
    echo -e "\nCan't access '$HTTP_REQ_HEADS_FILE'. Exiting.\n" >&2
    exit 1
fi

[ -z "$_24_HRS" ] && declare -r _24_HRS=$(( 60 * 60 * 24 ))

if (( _24_HRS < ( $( date +%s ) - $( stat -c %Y "$HTTP_REQ_HEADS_FILE" ) ) )) \
                        || \
   (( 3 > $( cat "$HTTP_REQ_HEADS_FILE" | wc -l ) ))
then

    HTTP_REQ_HEADS=""
    cat /dev/null > "$HTTP_REQ_HEADS_FILE"

elif [ -z "$HTTP_REQ_HEADS" ]
then
    setHttpRequestHeaders
fi

if [ -n "$HTTP_REQ_HEADS" ]
then
    # This script can be sourced or executed. Trying to put the
    # return and exit in the same expression mostly creates heartache.
    return &>/dev/null
    exit
fi


# Script only gets to this point if HTTP_REQ_HEADS_FILE needs refreshed.


for e in chromium grep mktemp ncat ps shuf ss tr
do
    if ! which $e &>/dev/null
    then
        echo -e "\n${0##*/} needs ${e}. Didn't find it. Exiting.\n" >&2
        exit 1
    fi
done


echo -e "\nRefreshing $HTTP_REQ_HEADS_FILE" >&2


function captureBrowsersDefaultHttpRequestHeaders
{
    declare -i port=1024
    declare -i ncPid=0

    while (( 32000 > port ))
    do
        port=1+port
        cat /etc/services | grep "$port/" &>/dev/null && \
                                            continue # Skip stdrd server ports.

        # Without -k the browser that opens to send a request
        # to ncat just hangs, along with this refresh process.
        ncat -lk 127.0.0.1 $port &> "$HTTP_REQ_HEADS_FILE" &
        ncPid=$!
        sleep .25             # Give ncat plenty of time to to init and listen.

        # Yes, -p. We own ncat and thus -p produces output.
        if ss -ltnp | grep ":$port" | grep "pid=$ncPid" >&2
        then
            break        # ncat is running on a port that's not already taken^.

        elif [ -n "$ncPid" ]      # If ncat asks for a taken port, there's a
        then                      # long delay before it dies^, and it will be
                                  # alive long after ss finishes. So kill it
                                  # here before next port attempt.
            echo "ncat failed to listen on ${port}" >&2
            kill -s SIGTERM $ncPid &>/dev/null
        fi
        # ^Unless the port's taken by another ncat instance, then both run,
        # both listening to the same port, with only one of them getting
        # incoming data.
    done

    if ! ps -hp $ncPid -o args &>/dev/null
    then
        echo -e "\nFailed to start ncat server instance. Exiting." >&2
        exit 1
    fi

    # Making a temp Chromium user data dir prevents it from opening a tab
    # inside of an already running Chromium instance, allowing it to be
    # cleanly closed independently of other already running instances.
    local tmpDir=$( mktemp -d )

    # TODO: When a Chromium with a headless mode becomes the norm, use it.
    $BROWSER --incognito --no-proxy-server --user-data-dir="$tmpDir" \
                    http://127.0.0.1:$port/SomeImage.jpg?foo=bar &>/dev/null &
    local browserPid=$!

    declare -i i=0
    while (( 200 > i )) && [ ! -s "$HTTP_REQ_HEADS_FILE" ]
    do
        i=i+1
        sleep .1
    done
    sleep .2                                  # Ensure all headers are written.

    if [ ! -s "$HTTP_REQ_HEADS_FILE" ]
    then
        echo -e "\nERROR: Didn't capture $BROWSER's HTTP req headers!\n" >&2
    fi

    kill -s SIGINT $browserPid &>/dev/null

    [ -d "$tmpDir" ] && rm -rf "$tmpDir" &>/dev/null

    kill -s SIGTERM $ncPid &>/dev/null
}

captureBrowsersDefaultHttpRequestHeaders
unset -f captureBrowsersDefaultHttpRequestHeaders
setHttpRequestHeaders

